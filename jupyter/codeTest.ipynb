{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 文本分类步骤\n",
    "- 划分数据集\n",
    "- 对标题和正文分词和去停用词\n",
    "- 计算tf-idf等特征\n",
    "- 构建分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从ES取出带标签的数据，分词，并dump到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\YDQing\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功导出新闻数据：size=103320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.042 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成对新闻标题的分词\n",
      "成功将分词后的数据dump到本地\n",
      "成功dump训练集到本地：size=82656\n",
      "成功dump测试集到本地：size=20664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'doc_title': ['南', '天', '信息', '管理层', '增持', '86', '万股'], 'doc_type': 'IT'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import esProxy\n",
    "from analyzer import Analyzer\n",
    "import pickle, random\n",
    "\n",
    "# 从ES导出带标签的新闻数据\n",
    "sougouNews = esProxy.getDataFromEs()\n",
    "print('成功导出新闻数据：size=%d' % (len(sougouNews)))\n",
    "\n",
    "def featurelize(sougouNews, fields=['doc_title'], analyzer=Analyzer()):\n",
    "    \"\"\"\n",
    "    返回标签和分词后的特征\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for doc in sougouNews:\n",
    "        dic = {}\n",
    "        for field in fields:\n",
    "            dic[field] = analyzer.cutAndFilter(doc[field])\n",
    "        # 添加新闻类别\n",
    "        dic['doc_type'] = doc['doc_type']\n",
    "        tokens.append(dic)\n",
    "    return tokens\n",
    "\n",
    "# 对新闻标题进行分词，得到带分词的新闻数据\n",
    "tokenSougouNews = featurelize(sougouNews, fields=['doc_title'], analyzer=Analyzer())\n",
    "print('完成对新闻标题的分词')\n",
    "\n",
    "# 将分词后的结果dump到本地\n",
    "with open('tokenSougouNews.pk', 'wb') as f:\n",
    "    f.truncate()\n",
    "    pickle.dump(tokenSougouNews, f)\n",
    "print('成功将分词后的数据dump到本地')\n",
    "\n",
    "# 划分训练集和测试集\n",
    "random.shuffle(tokenSougouNews)\n",
    "trainPercent = 0.8\n",
    "# dump训练集\n",
    "with open('tokenSougouNews-train.pk', 'wb') as f:\n",
    "    f.truncate()\n",
    "    pickle.dump(tokenSougouNews[:int(trainPercent*len(tokenSougouNews))], f)\n",
    "print('成功dump训练集到本地：size=%d' % (int(trainPercent*len(tokenSougouNews))))\n",
    "    \n",
    "# dump测试集\n",
    "with open('tokenSougouNews-test.pk', 'wb') as f:\n",
    "    f.truncate()\n",
    "    pickle.dump(tokenSougouNews[int(trainPercent*len(tokenSougouNews)):], f)\n",
    "print('成功dump测试集到本地：size=%d' % (len(tokenSougouNews) - int(trainPercent*len(tokenSougouNews))))\n",
    "\n",
    "tokenSougouNews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### tf-idf + 分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size=82656\n",
      "test size=20664\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenSougouNews-train.pk', 'rb') as f:\n",
    "    trainData = pickle.load(f)\n",
    "trainX = [dict(doc_title=' '.join(d['doc_title'])) for d in trainData]\n",
    "trainY = [d['doc_type'] for d in trainData]\n",
    "print('train size=%d' % (len(trainX)))\n",
    "    \n",
    "with open('tokenSougouNews-test.pk', 'rb') as f:\n",
    "    testData = pickle.load(f)\n",
    "testX = [dict(doc_title=' '.join(d['doc_title'])) for d in testData]\n",
    "testY = [d['doc_type'] for d in testData]\n",
    "print('test size=%d' % (len(testX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将文本tf-idf向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "import numpy as np\n",
    "\n",
    "class TfidfVectorizor(object):\n",
    "    def __init__(self, fields):\n",
    "        \"\"\"\n",
    "        fields: 需要向量化的属性\n",
    "        \"\"\"\n",
    "        self.fields = fields\n",
    "        self.tfidfVectorizors = dict()\n",
    "        for field in fields:\n",
    "            self.tfidfVectorizors[field] = text.TfidfVectorizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for field in self.fields:\n",
    "            docs = [x[field] for x in X]\n",
    "            self.tfidfVectorizors[field].fit(docs, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        将每个属性向量化后，拼接成一个向量\n",
    "        \"\"\"\n",
    "        vectors = None\n",
    "        for i, field in enumerate(self.fields):\n",
    "            docs = [x[field] for x in X]\n",
    "            vector = self.tfidfVectorizors[field].transform(docs)\n",
    "            vectors = np.hstack(vectors, vector) if i > 0 else vector\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda34\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1015: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf+multiNB: trainAcc=0.867886, testAcc=0.821235\n",
      "tfidf+svm: trainAcc=0.981018, testAcc=0.895906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nbClf = Pipeline([('tfidfVectorizor', TfidfVectorizor(['doc_title'])),\n",
    "                 ('multinomialNB', MultinomialNB())])\n",
    "nbClf.fit(trainX, trainY)\n",
    "\n",
    "# 计算误差\n",
    "trainAcc = accuracy_score(trainY, nbClf.predict(trainX))\n",
    "testAcc = accuracy_score(testY, nbClf.predict(testX))\n",
    "print('tfidf+multiNB: trainAcc=%f, testAcc=%f' % (trainAcc, testAcc))\n",
    "\n",
    "svmClf = Pipeline([('tfidfVectorizor', TfidfVectorizor(['doc_title'])),\n",
    "                 ('svm', LinearSVC())])\n",
    "svmClf.fit(trainX, trainY)\n",
    "\n",
    "# 计算误差\n",
    "trainAcc = accuracy_score(trainY, svmClf.predict(trainX))\n",
    "testAcc = accuracy_score(testY, svmClf.predict(testX))\n",
    "print('tfidf+svm: trainAcc=%f, testAcc=%f' % (trainAcc, testAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### word2vec向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "C:\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('国际足球', 0.9797388911247253),\n",
       " ('搜狐', 0.9744399189949036),\n",
       " ('日内瓦', 0.9741291999816895),\n",
       " ('广汽传祺', 0.9712013006210327),\n",
       " ('专访', 0.967939019203186),\n",
       " ('作文题', 0.967909574508667),\n",
       " ('乔氏杯', 0.9676322937011719),\n",
       " ('卫视', 0.963369607925415),\n",
       " ('篮联', 0.9627541303634644),\n",
       " ('逐题', 0.9621201753616333)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Doc2VecVectorizor(object):\n",
    "    def __init__(self, fields, size=200, window=3, min_count=1):\n",
    "        self.fields = fields\n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.word2vec = Word2Vec(size=size, window=window, min_count=min_count)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        sentences = []\n",
    "        for x in X:\n",
    "            for field in self.fields:\n",
    "                sentences.append(x[field].split())\n",
    "        self.word2vec.build_vocab(sentences)\n",
    "        self.word2vec.train(sentences, total_examples=self.word2vec.corpus_count,epochs=self.word2vec.iter)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        计算文档的特征向量\n",
    "        1. 对每个属性，计算每个词的vector，然后将所有词的vector的平均值作为该属性的vector\n",
    "        2. 所有属性的vector，flatten为一个宽vector，作为该文档的特征向量\n",
    "        \"\"\"\n",
    "        return np.array([self.__doc2vec(x) for x in X])\n",
    "        \n",
    "    def __sentence2vec(self, sentence):\n",
    "        if sentence.strip() == 0:\n",
    "            return np.zeros(self.size)\n",
    "        vectors = [self.word2vec[word] if word in self.word2vec else np.zeros(self.size) for word in sentence.split()]\n",
    "        return np.mean(vectors, axis=0)\n",
    "    \n",
    "    def __doc2vec(self, doc):\n",
    "        vectors = np.array([self.__sentence2vec(doc[field]) for field in self.fields])\n",
    "        return vectors.flatten()\n",
    "    \n",
    "doc2vec = Doc2VecVectorizor(fields=['doc_title'])\n",
    "doc2vec.fit(trainX)\n",
    "doc2vec.transform([dict(doc_title='北京 奥运会')])\n",
    "doc2vec.word2vec.wv.similar_by_word(word='体育', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec + svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    }
   ],
   "source": [
    "svmClf = Pipeline([('doc2vec', Doc2VecVectorizor(['doc_title'])),\n",
    "                 ('svm', LinearSVC())])\n",
    "svmClf.fit(trainX, trainY)\n",
    "\n",
    "# 计算误差\n",
    "trainAcc = accuracy_score(trainY, svmClf.predict(trainX))\n",
    "testAcc = accuracy_score(testY, svmClf.predict(testX))\n",
    "print('doc2vec+svm: trainAcc=%f, testAcc=%f' % (trainAcc, testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Anaconda34\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Anaconda34\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-4e3047e66fdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msvmClf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m14810\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m14820\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda34\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda34\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'_final_estimator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda34\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda34\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    242\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda34\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    371\u001b[0m                                       force_all_finite)\n\u001b[0;32m    372\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "svmClf.predict(testX[14810:14820])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_title': '天价 粽子 调查 1880 元 故宫 督造 礼盒 最 畅销'},\n",
       " {'doc_title': '中国围棋 电视 快棋赛 柁 嘉熹 夺冠 下 月 出战 亚洲杯'},\n",
       " {'doc_title': '图文 中八 郑州 站 预选赛 落幕 03 裁判 宣布 成绩'},\n",
       " {'doc_title': ''},\n",
       " {'doc_title': '27 省 公布 上半年 居民 人均收入 全部 跑 赢 CPI'},\n",
       " {'doc_title': '1 布雷 西 VS 科莫'},\n",
       " {'doc_title': '图文 总决赛 女排 VS 土耳其 惠若琪 被 拦'},\n",
       " {'doc_title': '2012 年 搜超 联赛 A 组 第一轮 体育 男女队 双双 取胜'},\n",
       " {'doc_title': '买家 竞购 标致 雪铁龙 拟售 捷富凯 物流 股权'},\n",
       " {'doc_title': '满足 个性化 需求 斯柯达 将 国产 晶锐 旅行 版'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX[14810:14820]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
