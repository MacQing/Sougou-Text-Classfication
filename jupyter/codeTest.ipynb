{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 文本分类步骤\n",
    "- 划分数据集\n",
    "- 对标题和正文分词和去停用词\n",
    "- 计算tf-idf等特征\n",
    "- 构建分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从ES取出带标签的数据，分词，并dump到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\YDQing\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功导出新闻数据：size=103320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.042 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成对新闻标题的分词\n",
      "成功将分词后的数据dump到本地\n",
      "成功dump训练集到本地：size=82656\n",
      "成功dump测试集到本地：size=20664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'doc_title': ['南', '天', '信息', '管理层', '增持', '86', '万股'], 'doc_type': 'IT'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import esProxy\n",
    "from analyzer import Analyzer\n",
    "import pickle, random\n",
    "\n",
    "# 从ES导出带标签的新闻数据\n",
    "sougouNews = esProxy.getDataFromEs()\n",
    "print('成功导出新闻数据：size=%d' % (len(sougouNews)))\n",
    "\n",
    "def featurelize(sougouNews, fields=['doc_title'], analyzer=Analyzer()):\n",
    "    \"\"\"\n",
    "    返回标签和分词后的特征\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for doc in sougouNews:\n",
    "        dic = {}\n",
    "        for field in fields:\n",
    "            dic[field] = analyzer.cutAndFilter(doc[field])\n",
    "        # 添加新闻类别\n",
    "        dic['doc_type'] = doc['doc_type']\n",
    "        tokens.append(dic)\n",
    "    return tokens\n",
    "\n",
    "# 对新闻标题进行分词，得到带分词的新闻数据\n",
    "tokenSougouNews = featurelize(sougouNews, fields=['doc_title'], analyzer=Analyzer())\n",
    "print('完成对新闻标题的分词')\n",
    "\n",
    "# 将分词后的结果dump到本地\n",
    "with open('tokenSougouNews.pk', 'wb') as f:\n",
    "    f.truncate()\n",
    "    pickle.dump(tokenSougouNews, f)\n",
    "print('成功将分词后的数据dump到本地')\n",
    "\n",
    "# 划分训练集和测试集\n",
    "random.shuffle(tokenSougouNews)\n",
    "trainPercent = 0.8\n",
    "# dump训练集\n",
    "with open('tokenSougouNews-train.pk', 'wb') as f:\n",
    "    f.truncate()\n",
    "    pickle.dump(tokenSougouNews[:int(trainPercent*len(tokenSougouNews))], f)\n",
    "print('成功dump训练集到本地：size=%d' % (int(trainPercent*len(tokenSougouNews))))\n",
    "    \n",
    "# dump测试集\n",
    "with open('tokenSougouNews-test.pk', 'wb') as f:\n",
    "    f.truncate()\n",
    "    pickle.dump(tokenSougouNews[int(trainPercent*len(tokenSougouNews)):], f)\n",
    "print('成功dump测试集到本地：size=%d' % (len(tokenSougouNews) - int(trainPercent*len(tokenSougouNews))))\n",
    "\n",
    "tokenSougouNews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### tf-idf + bayes分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size=82656\n",
      "test size=20664\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenSougouNews-train.pk', 'rb') as f:\n",
    "    trainData = pickle.load(f)\n",
    "trainX = [dict(doc_title=' '.join(d['doc_title'])) for d in trainData]\n",
    "trainY = [d['doc_type'] for d in trainData]\n",
    "print('train size=%d' % (len(trainX)))\n",
    "    \n",
    "with open('tokenSougouNews-test.pk', 'rb') as f:\n",
    "    testData = pickle.load(f)\n",
    "testX = [dict(doc_title=' '.join(d['doc_title'])) for d in testData]\n",
    "testY = [d['doc_type'] for d in testData]\n",
    "print('test size=%d' % (len(testX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将文本tf-idf向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "import numpy as np\n",
    "\n",
    "class TfidfVectorizor(object):\n",
    "    def __init__(self, fields):\n",
    "        \"\"\"\n",
    "        fields: 需要向量化的属性\n",
    "        \"\"\"\n",
    "        self.fields = fields\n",
    "        self.tfidfVectorizors = dict()\n",
    "        for field in fields:\n",
    "            self.tfidfVectorizors[field] = text.TfidfVectorizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for field in self.fields:\n",
    "            docs = [x[field] for x in X]\n",
    "            self.tfidfVectorizors[field].fit(docs, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        将每个属性向量化后，拼接成一个向量\n",
    "        \"\"\"\n",
    "        vectors = None\n",
    "        for i, field in enumerate(self.fields):\n",
    "            docs = [x[field] for x in X]\n",
    "            vector = self.tfidfVectorizors[field].transform(docs)\n",
    "            vectors = np.hstack(vectors, vector) if i > 0 else vector\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda34\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1015: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf+multiNB: trainAcc=0.867886, testAcc=0.821235\n",
      "tfidf+svm: trainAcc=0.981018, testAcc=0.895906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nbClf = Pipeline([('tfidfVectorizor', TfidfVectorizor(['doc_title'])),\n",
    "                 ('multinomialNB', MultinomialNB())])\n",
    "nbClf.fit(trainX, trainY)\n",
    "\n",
    "# 计算误差\n",
    "trainAcc = accuracy_score(trainY, nbClf.predict(trainX))\n",
    "testAcc = accuracy_score(testY, nbClf.predict(testX))\n",
    "print('tfidf+multiNB: trainAcc=%f, testAcc=%f' % (trainAcc, testAcc))\n",
    "\n",
    "svmClf = Pipeline([('tfidfVectorizor', TfidfVectorizor(['doc_title'])),\n",
    "                 ('svm', LinearSVC())])\n",
    "svmClf.fit(trainX, trainY)\n",
    "\n",
    "# 计算误差\n",
    "trainAcc = accuracy_score(trainY, svmClf.predict(trainX))\n",
    "testAcc = accuracy_score(testY, svmClf.predict(testX))\n",
    "print('tfidf+svm: trainAcc=%f, testAcc=%f' % (trainAcc, testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
