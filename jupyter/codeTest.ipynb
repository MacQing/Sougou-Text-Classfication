{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 文本分类步骤\n",
    "- 划分数据集\n",
    "- 对标题和正文分词和去停用词\n",
    "- 计算tf-idf等特征\n",
    "- 构建分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从ES取出带标签的数据，分词，并dump到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\YDQing\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功导出新闻数据：size=103320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.214 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成对新闻标题的分词\n",
      "成功将分词后的数据dump到本地\n",
      "成功dump训练集到本地：size=82656\n",
      "成功dump测试集到本地：size=20664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'doc_content': ['本期',\n",
       "  '节目',\n",
       "  '内容',\n",
       "  '介绍',\n",
       "  '关注',\n",
       "  '机动车',\n",
       "  '驾驶证',\n",
       "  '申领',\n",
       "  '和',\n",
       "  '使用',\n",
       "  '规定',\n",
       "  '搜狐',\n",
       "  '汽车',\n",
       "  '广播',\n",
       "  '诚邀',\n",
       "  '全国',\n",
       "  '各地',\n",
       "  '强势',\n",
       "  '电台',\n",
       "  '真情',\n",
       "  '加盟',\n",
       "  '携手',\n",
       "  '打造',\n",
       "  '中国',\n",
       "  '汽车',\n",
       "  '广播',\n",
       "  '最强',\n",
       "  '容',\n",
       "  '把脉',\n",
       "  '全球',\n",
       "  '汽车产业',\n",
       "  '风向标',\n",
       "  '引领',\n",
       "  '时尚',\n",
       "  '汽车',\n",
       "  '消费',\n",
       "  '的',\n",
       "  '参考书',\n",
       "  '搜狐',\n",
       "  '汽车',\n",
       "  '广播',\n",
       "  '车旅',\n",
       "  '杂志',\n",
       "  '服务',\n",
       "  '我们',\n",
       "  '的',\n",
       "  '汽车',\n",
       "  '生活',\n",
       "  '加盟',\n",
       "  '热线',\n",
       "  '13381202220',\n",
       "  '010',\n",
       "  '62729907',\n",
       "  '独家',\n",
       "  '出品',\n",
       "  '搜狐',\n",
       "  '汽车',\n",
       "  '事业部'],\n",
       " 'doc_title': ['搜狐', '汽车', '广播', '车旅', '杂志', '2012', '06', '20', '期'],\n",
       " 'doc_type': '汽车'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import esProxy\n",
    "from analyzer import Analyzer\n",
    "import pickle, random\n",
    "\n",
    "# 从ES导出带标签的新闻数据\n",
    "sougouNews = esProxy.getDataFromEs()\n",
    "print('成功导出新闻数据：size=%d' % (len(sougouNews)))\n",
    "\n",
    "def featurelize(sougouNews, fields=['doc_title'], analyzer=Analyzer()):\n",
    "    \"\"\"\n",
    "    返回标签和分词后的特征\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for doc in sougouNews:\n",
    "        dic = {}\n",
    "        for field in fields:\n",
    "            dic[field] = analyzer.cutAndFilter(doc[field])\n",
    "        # 添加新闻类别\n",
    "        dic['doc_type'] = doc['doc_type']\n",
    "        tokens.append(dic)\n",
    "    return tokens\n",
    "\n",
    "# 对新闻标题进行分词，得到带分词的新闻数据\n",
    "tokenSougouNews = featurelize(sougouNews, fields=['doc_title', 'doc_content'], analyzer=Analyzer())\n",
    "print('完成对新闻标题的分词')\n",
    "\n",
    "# 将分词后的结果dump到本地\n",
    "with open('tokenSougouNews.pk', 'wb') as f:\n",
    "    f.truncate()\n",
    "    pickle.dump(tokenSougouNews, f)\n",
    "print('成功将分词后的数据dump到本地')\n",
    "\n",
    "# 划分训练集和测试集\n",
    "random.shuffle(tokenSougouNews)\n",
    "trainPercent = 0.8\n",
    "# dump训练集\n",
    "with open('tokenSougouNews-train.pk', 'wb') as f:\n",
    "    f.truncate()\n",
    "    pickle.dump(tokenSougouNews[:int(trainPercent*len(tokenSougouNews))], f)\n",
    "print('成功dump训练集到本地：size=%d' % (int(trainPercent*len(tokenSougouNews))))\n",
    "    \n",
    "# dump测试集\n",
    "with open('tokenSougouNews-test.pk', 'wb') as f:\n",
    "    f.truncate()\n",
    "    pickle.dump(tokenSougouNews[int(trainPercent*len(tokenSougouNews)):], f)\n",
    "print('成功dump测试集到本地：size=%d' % (len(tokenSougouNews) - int(trainPercent*len(tokenSougouNews))))\n",
    "\n",
    "tokenSougouNews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### tf-idf + 分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size=82656\n",
      "test size=20664\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "    \n",
    "with open('tokenSougouNews-test.pk', 'rb') as f:\n",
    "    testData = pickle\n",
    "with open('tokenSougouNews-train.pk', 'rb') as f:\n",
    "    trainData = pickle.load(f)\n",
    "trainX = [dict(doc_title=' '.join(d['doc_title'])) for d in trainData]\n",
    "trainY = [d['doc_type'] for d in trainData]\n",
    "print('train size=%d' % (len(trainX))).load(f)\n",
    "testX = [dict(doc_title=' '.join(d['doc_title'])) for d in testData]\n",
    "testY = [d['doc_type'] for d in testData]\n",
    "print('test size=%d' % (len(testX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将文本tf-idf向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "import numpy as np\n",
    "\n",
    "class TfidfVectorizor(object):\n",
    "    def __init__(self, fields):\n",
    "        \"\"\"\n",
    "        fields: 需要向量化的属性\n",
    "        \"\"\"\n",
    "        self.fields = fields\n",
    "        self.tfidfVectorizors = dict()\n",
    "        for field in fields:\n",
    "            self.tfidfVectorizors[field] = text.TfidfVectorizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for field in self.fields:\n",
    "            docs = [x[field] for x in X]\n",
    "            self.tfidfVectorizors[field].fit(docs, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        将每个属性向量化后，拼接成一个向量\n",
    "        \"\"\"\n",
    "        vectors = None\n",
    "        for i, field in enumerate(self.fields):\n",
    "            docs = [x[field] for x in X]\n",
    "            vector = self.tfidfVectorizors[field].transform(docs)\n",
    "            vectors = np.hstack(vectors, vector) if i > 0 else vector\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda34\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1015: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf+lr: trainAcc=0.913848, testAcc=0.869774\n",
      "tfidf+multiNB: trainAcc=0.867886, testAcc=0.821235\n",
      "tfidf+svm: trainAcc=0.981018, testAcc=0.895906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tfidf + lr\n",
    "lrClf = Pipeline([('tfidfVectorizor', TfidfVectorizor(['doc_title'])),\n",
    "                 ('lr', LogisticRegression())])\n",
    "lrClf.fit(trainX, trainY)\n",
    "\n",
    "trainAcc = accuracy_score(trainY, lrClf.predict(trainX))\n",
    "testAcc = accuracy_score(testY, lrClf.predict(testX))\n",
    "print('tfidf+lr: trainAcc=%f, testAcc=%f' % (trainAcc, testAcc))\n",
    "\n",
    "# tfidf + nb\n",
    "nbClf = Pipeline([('tfidfVectorizor', TfidfVectorizor(['doc_title'])),\n",
    "                 ('multinomialNB', MultinomialNB())])\n",
    "nbClf.fit(trainX, trainY)\n",
    "\n",
    "trainAcc = accuracy_score(trainY, nbClf.predict(trainX))\n",
    "testAcc = accuracy_score(testY, nbClf.predict(testX))\n",
    "print('tfidf+multiNB: trainAcc=%f, testAcc=%f' % (trainAcc, testAcc))\n",
    "\n",
    "# tfidf + svm\n",
    "svmClf = Pipeline([('tfidfVectorizor', TfidfVectorizor(['doc_title'])),\n",
    "                 ('svm', LinearSVC())])\n",
    "svmClf.fit(trainX, trainY)\n",
    "\n",
    "trainAcc = accuracy_score(trainY, svmClf.predict(trainX))\n",
    "testAcc = accuracy_score(testY, svmClf.predict(testX))\n",
    "print('tfidf+svm: trainAcc=%f, testAcc=%f' % (trainAcc, testAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### word2vec向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Doc2VecVectorizor at 0x1fd5007cf98>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Doc2VecVectorizor(object):\n",
    "    def __init__(self, fields, size=100, window=3, min_count=1):\n",
    "        self.fields = fields\n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.word2vec = Word2Vec(size=size, window=window, min_count=min_count)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        sentences = []\n",
    "        for x in X:\n",
    "            for field in self.fields:\n",
    "                sentences.append(x[field].split())\n",
    "        self.word2vec.build_vocab(sentences)\n",
    "        self.word2vec.train(sentences, total_examples=self.word2vec.corpus_count,epochs=self.word2vec.iter)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        计算文档的特征向量\n",
    "        1. 对每个属性，计算每个词的vector，然后将所有词的vector的平均值作为该属性的vector\n",
    "        2. 所有属性的vector，flatten为一个宽vector，作为该文档的特征向量\n",
    "        \"\"\"\n",
    "        return np.array([self.__doc2vec(x) for x in X])\n",
    "        \n",
    "    def __sentence2vec(self, sentence):\n",
    "        if len(sentence.strip()) == 0:\n",
    "            return np.zeros(self.size)\n",
    "        vectors = [self.word2vec[word] if word in self.word2vec else np.zeros(self.size) for word in sentence.split()]\n",
    "        return np.mean(vectors, axis=0)\n",
    "    \n",
    "    def __doc2vec(self, doc):\n",
    "        vectors = np.array([self.__sentence2vec(doc[field]) for field in self.fields])\n",
    "        return vectors.flatten()\n",
    "    \n",
    "doc2vec = Doc2VecVectorizor(fields=['doc_title'])\n",
    "doc2vec.fit(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('舞蹈节', 0.9734185934066772),\n",
       " ('专访', 0.9699808955192566),\n",
       " ('老年人', 0.9686485528945923),\n",
       " ('日内瓦', 0.9671200513839722),\n",
       " ('搜狐', 0.9666953086853027),\n",
       " ('看车', 0.963032603263855),\n",
       " ('国际足球', 0.9596318006515503),\n",
       " ('广汽传祺', 0.9582968950271606),\n",
       " ('篮联', 0.9582201242446899),\n",
       " ('海河', 0.9577779173851013)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.word2vec.wv.similar_by_word(word='体育', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.word2vec.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec + svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "C:\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec+svm: trainAcc=0.705841, testAcc=0.708672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "svmClf = Pipeline([('doc2vec', Doc2VecVectorizor(['doc_title'])),\n",
    "                 ('svm', LinearSVC())])\n",
    "svmClf.fit(trainX, trainY)\n",
    "\n",
    "# 计算误差\n",
    "trainAcc = accuracy_score(trainY, svmClf.predict(trainX))\n",
    "testAcc = accuracy_score(testY, svmClf.predict(testX))\n",
    "print('doc2vec+svm: trainAcc=%f, testAcc=%f' % (trainAcc, testAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf加权的word2vec + classification\n",
    "#### tf-idf加权的word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Doc2VecVectorizor(object):\n",
    "    def __init__(self, tfidfVectorizor, word2vecVectorizor, fields):\n",
    "        self.tfidfVectorizor = tfidfVectorizor\n",
    "        self.word2vecVectorizor = word2vecVectorizor\n",
    "        self.fields = fields\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        计算文档的特征向量\n",
    "        1. 对每个属性，计算每个词的tfidf-vector和word-vector，然后将所有词的两个vector的加权平均向量作为该属性的vector\n",
    "        2. 所有属性的vector，flatten为一个宽vector，作为该文档的特征向量\n",
    "        \"\"\"\n",
    "        return np.array([self.__doc2vec(x) for x in X])\n",
    "        \n",
    "    def __sentence2vec(self, sentence):\n",
    "        if len(sentence.strip()) == 0:\n",
    "            return np.zeros(self.size)\n",
    "        vectors = [self.word2vecVectorizor[word]*self.tfidfVectorizor.transform() \n",
    "                   if word in self.word2vecVectorizor else np.zeros(self.size) \n",
    "                   for word in sentence.split()]\n",
    "        return np.mean(vectors, axis=0)\n",
    "    \n",
    "    def __doc2vec(self, doc):\n",
    "        vectors = np.array([self.__sentence2vec(doc[field]) for field in self.fields])\n",
    "        return vectors.flatten()\n",
    "    \n",
    "doc2vec = Doc2VecVectorizor(fields=['doc_title'])\n",
    "doc2vec.fit(trainX)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
